#!/bin/bash

# Copyright 2019 ThoughtSpot
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation
# files (the "Software"), to deal in the Software without restriction, including without limitation the rights to use, copy,
# modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the
# Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES
# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
# BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT
# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

# This file defines all of the settings for running a single load from a directory (and optionally sub-directories) using the
# load_files bash script.  You should be able to modify this file and use with load_file with no additonal modification.

# NOTE that the variables below are generally assumed to exist.  Deleting a variable rather than simply changing the value may
# cause errors in the load script.

declare -r SOURCE_TYPE="file_system"                     # can be one of "file_system" or "aws_s3".  Default is "file_system".
#declare -r SOURCE_TYPE="aws_s3"                         # can be one of "file_system" or "aws_s3".  Default is "file_system".

declare -r DATA_FILE_EXTENSION=".csv"                    # extension for files to be loaded.
declare -r ROOT_DIR="/home/admin"                        # Root where files for loading are located.
declare -r RESULTS_EMAIL=("user.name@company.com")       # email addresses to send the results to.
declare -r DATABASE_NAME="my_db"                         # database name.  Only one currently support per config.
declare -r DEFAULT_SCHEMA_NAME="falcon_default_schema"   # default schema name.
declare -r IGNORE_DIRS=(loaded test old file_backup)     # list of directories to ignore files to load.
declare -r NBR_DAYS_TO_KEEP_OLD_FILES=7                  # must be integer, set to blank to not remove old archives.
declare -r MOVE_LOADED_FILES="mv"                        # Command for files that were loaded.  Either "mv" or "cp" or "echo".
declare -r TRUNCATE_BEFORE_LOAD="false"                  # If "true", tables will be truncated when first loaded.  Useful if parts exist.
declare -r ARCHIVE_DATA="always"                         # Whether the data should be archived in the old folder. 
                                                         # For large data sets this can take considerable time and space. Valid options: 
                                                         # - always     : Files are always archived, tarred etc
                                                         # - onerror    : Files are archived when one of the files in the load failed
                                                         # - onbad      : Files are archived when one of the files generated bad records
                                                         # - never      : Files are never archived

# Settings specific for file_system loads.
declare -r DATA_DIR=${ROOT_DIR}/data                     # location for the data to load.  Write files to load here.

# Settings specific for AWS S3 loads.
# If using AWS S3 as a source, set these flags.  The DATA_DIR from above will not be used and you should use the AWS_S3_DATA_DIR instead.
declare -r AWS_S3_ACCESS_KEY_ID="ACCESS_KEY_FROM_AWS_CONFIG"
declare -r AWS_S3_SECRET_ACCESS_KEY="SECRET_ACCESS_KEY_FROM_AWS_CONFIG"
declare -r AWS_S3_REGION="us-west-2"                      # regions here:  https://docs.aws.amazon.com/general/latest/gr/rande.html
declare -r AWS_S3_BUCKET="load-test"                      # name of the bucket for files to load.
declare -r AWS_S3_ARCHIVE_BUCKET="load-test-archive"      # name of the bucket to save loaded files.  If blank no archive will happen.
declare -r AWS_S3_DATA_DIR="/"                            # folder under the bucket to start loading from.  This is the actual data root.
declare -r AWS_S3_BUF_CAPACITY=134217728                  # buffer for loading files

# if using a semaphore file to control the start, set it here.  Set to "" if not using.
declare -r SEMAPHORE_FILE_NAME="stage_done"              # name of a file that indicates all staging files are loaded.

# Set any parts of the names that will be stripped from the file name to determine the table name. Patters separated by spaces.
SED_PATTERNS=("delete_me" "and_me")

# Get the cluster name for cluster specific logging.
if hash tscli 2>/dev/null; then
  declare -r CLUSTER_NAME="`tscli cluster status | grep 'Cluster name' | sed 's/^.*: //' | sed 's/ /_/'`"
else
  declare -r CLUSTER_NAME="no_cluster_on_`hostname -s`"
fi

# TSLOAD flags.
# TODO set the flags for your environment.  The assumption is that all files have the same format.  If you have different
# formats, consider multiple configuration files and loads.
#declare -r DEFAULT_EMPTY_TARGET="--empty_target"                      # either --empty_target or ""
declare -r DEFAULT_EMPTY_TARGET=""                                     # either --empty_target or ""
declare -r SOURCE_DATA_FORMAT=csv                                      # either csv or delimited.
declare -r FIELD_SEPARATOR="|"                                         # field separator in the data.
declare -r MAX_IGNORED_ROWS=0                                          # maximum rows to ignore.  0 recommended for production.
declare -r HAS_HEADER_ROW="true"                                       # true if there is a header row to ignore.
declare -r NULL_VALUE=""                                               # value in the data for NULLs
declare -r DATE_FORMAT="%m-%d-%Y"                                      # format for parsing dates
declare -r DATE_TIME_FORMAT="%m-%d-%Y %H:%M:%S"                        # format for parsing date/times
declare -r BOOLEAN_REPRESENTATION="True_False"                         # Boolean representation as TRUE_FALSE

# The following can be modified, but typically don't need to be.
declare -r THE_DATE_TIME="`date +"%Y-%m-%d_%H%M%S_%Z"`"                # date and time to use for load name.
declare -r LOG_DIR=${ROOT_DIR}/logs                                    # directory to store load logs.
declare -r TEMP_TSLOAD_FILE="/tmp/$$.tsload.out"                       # temp file for capturing tsload output per file
declare -r TEMP_RESULTS_FILE="/tmp/$$.results"                         # temp file for detailed results.
declare -r TEMP_STATS_FILE="/tmp/$$.tsload_stats.txt"                  # temp file for loading stats.
declare -r TEMP_RESULTS_SUMMARY_FILE="/tmp/$$.results.summary"         # temp folder for results summary.
declare -r RESULTS_FILE="${LOG_DIR}/${CLUSTER_NAME}-results-${THE_DATE_TIME}.txt" # stores the results of the data load.
declare -r LOADING_FILE="${ROOT_DIR}/loading"                          # flag to indicate we are currently loading.
declare -r V_LEVEL=0                                                   # logging verbosity level.  0-6.  0 recommended for production.
declare -r USE_HTML_EMAIL=1                                            # flag (0=false, 1=true) whether to send email in html format

if [[ ${SOURCE_TYPE} == "aws_s3" ]]; then
  declare -r OLD_DIR_ROOT="s3://${AWS_S3_ARCHIVE_BUCKET}"              # root location for copying loaded files.
  declare -r OLD_DIR="${OLD_DIR_ROOT}/${THE_DATE_TIME}"                # directory to store loaded files.
else # default to file system
  declare -r OLD_DIR_ROOT=${ROOT_DIR}/old                              # the 'root' folder for the old_dir (only used for clean-up)
  declare -r OLD_DIR="${OLD_DIR_ROOT}/${THE_DATE_TIME}"                # directory to store loaded files and bad records to.
fi

# queries to run before loading a table
archive_date=`date --date="7 days ago" +%Y-%m-%d`
declare -A table_queries=()
#declare -A table_queries=(
#        ["the_table"]="delete from the_table where date_id>='$archive_date'"
#)

# The following two "extra_table" sections allow you to specify extra columns to add to the data when doing file_system loads.  
# This capability is helpful for scenarios where you want to provide default values for columns that are not in the data.  
# Currently the data is hard coded.  Update each section for the tables you want to add.

# Defines a mapping of table names to additional headers to add.  Should match values.
declare -A extra_table_headers=()
# declare -A extra_table_headers=(
#   ["test_table1"]="new_column1"
#   ["test_table2"]="new_column2${FIELD_SEPARATOR}new_column3"
# )

# Defines a mapping of table names to additional values to add.  Should match headers.
declare -A extra_table_values=()
# declare -A extra_table_values=(
#   ["test_table1"]="new_value1"
#   ["test_table2"]="new_value2${FIELD_SEPARATOR}new_value3"
# )

